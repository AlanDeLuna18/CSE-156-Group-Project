{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install datasets\n",
        "# !pip install rouge_score"
      ],
      "metadata": {
        "id": "-Sx5ep26bP20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O5wwvMPRjlhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "class SelfRefine:\n",
        "    def __init__(self, tokenizer, model, f_model):\n",
        "        self.original_text = None\n",
        "        self.refined_text = None\n",
        "        self.previous_refined_text = None\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.feedback = None\n",
        "        self.feedback_model = f_model\n",
        "\n",
        "    # Generate input text\n",
        "    def generator(self, file_name):\n",
        "        with open(file_name, 'r', encoding='utf-8') as file:\n",
        "            self.original_text = [line.strip() for line in file]\n",
        "        print(\"Original text loaded.\")\n",
        "\n",
        "    # Feedback\n",
        "    def generate_feedback(self):\n",
        "        if not self.refined_text and not self.original_text:\n",
        "            raise ValueError(\"No text available to generate feedback.\")\n",
        "\n",
        "        # Use refined text if available, otherwise use original_text\n",
        "        text_to_feedback = self.refined_text if self.refined_text else self.original_text\n",
        "\n",
        "        # Get feedback\n",
        "        issues = feedback_model(text_to_feedback)\n",
        "        if self.previous_refined_text and self.previous_refined_text == text_to_feedback:\n",
        "            feedback = \"No significant improvement detected. Try rephrasing or restructuring the content.\"\n",
        "        else:\n",
        "            feedback = f\"Detected Issue: {issues[0]['label']}. Consider revising.\"\n",
        "\n",
        "        self.feedback = feedback\n",
        "\n",
        "        print(\"Generated feedback.\")\n",
        "        return feedback\n",
        "\n",
        "    # Refinement\n",
        "    def refinement(self):\n",
        "        if not self.original_text:\n",
        "            raise ValueError(\"No original text found. Run generator first.\")\n",
        "\n",
        "        # Choose text to refine\n",
        "        if self.refined_text:\n",
        "            self.previous_refined_text = self.refined_text[:]\n",
        "        else:\n",
        "            self.previous_refined_text = self.original_text[:]\n",
        "\n",
        "        # Add feedback\n",
        "        if self.feedback:\n",
        "            text_to_refine = [f\"{text} [Feedback: {self.feedback}]\" for text in self.previous_refined_text]\n",
        "        else:\n",
        "            text_to_refine = self.previous_refined_text\n",
        "\n",
        "        # Tokenize the text\n",
        "        inputs = self.tokenizer(text_to_refine, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
        "\n",
        "        # Generate refined text\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(**inputs, max_length=512)\n",
        "\n",
        "        # Decode the refined text\n",
        "        self.refined_text = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "        print(\"Refined text generated.\")\n",
        "\n",
        "    # Evaluation metric 1\n",
        "    def calculate_bleu(self, target, generated):\n",
        "        reference = [t.split() for t in target]\n",
        "        hypothesis = [g.split() for g in generated]\n",
        "        score = corpus_bleu(reference, hypothesis, smoothing_function=SmoothingFunction().method1)\n",
        "        return score\n",
        "\n",
        "    # Evaluation metric 2\n",
        "    def calculate_rouge(self, target, generated):\n",
        "        scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "        scores = [scorer.score(t, g) for t, g in zip(target, generated)]\n",
        "        return sum(s['rougeL'].fmeasure for s in scores) / len(scores)\n",
        "\n",
        "# Main loop\n",
        "def main(file_name, tokenizer, model, cycles, f_model):\n",
        "    test = SelfRefine(tokenizer=tokenizer, model=model, f_model=f_model)\n",
        "\n",
        "    # Load original text\n",
        "    test.generator(file_name)\n",
        "\n",
        "    # Run feedback and refinement cycles\n",
        "    for c in range(cycles):\n",
        "        print(f\"Cycle {c + 1}: \")\n",
        "        feedback = test.generate_feedback()\n",
        "        print(f\"Feedback: {feedback}\")\n",
        "        test.refinement()\n",
        "\n",
        "    # Final results\n",
        "    print(f\"Original text: {test.original_text[:1]}...\")\n",
        "    print(f\"Final feedback: {test.feedback}\")\n",
        "    print(f\"Final refined text: {test.refined_text[:1]}...\")\n",
        "\n",
        "    # Evaluate\n",
        "    rouge_score = test.calculate_rouge(test.original_text, test.refined_text)\n",
        "    # bleu_score = test.calculate_bleu(test.original_text, test.refined_text)\n",
        "    print(f\"Rouge Score: {rouge_score}\")\n",
        "\n",
        "# Load model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "feedback_model = pipeline(\"text-classification\", model=\"textattack/roberta-base-CoLA\")\n",
        "\n",
        "# Run test\n",
        "main(file_name='moby_dick.txt', tokenizer=tokenizer, model=model, cycles=5, f_model=feedback_model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKJ4tTuLjkm9",
        "outputId": "64198bcd-c211-4590-94f2-1169d1d054d8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at textattack/roberta-base-CoLA were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text loaded.\n",
            "Cycle 1: \n",
            "Generated feedback.\n",
            "Feedback: Detected Issue: LABEL_0. Consider revising.\n",
            "Refined text generated.\n",
            "Cycle 2: \n",
            "Generated feedback.\n",
            "Feedback: Detected Issue: LABEL_0. Consider revising.\n",
            "Refined text generated.\n",
            "Cycle 3: \n",
            "Generated feedback.\n",
            "Feedback: Detected Issue: LABEL_0. Consider revising.\n",
            "Refined text generated.\n",
            "Cycle 4: \n",
            "Generated feedback.\n",
            "Feedback: Detected Issue: LABEL_0. Consider revising.\n",
            "Refined text generated.\n",
            "Cycle 5: \n",
            "Generated feedback.\n",
            "Feedback: Detected Issue: LABEL_0. Consider revising.\n",
            "Refined text generated.\n",
            "Original text: ['Call me Ishmael. Some years ago—never mind how long precisely—having little or no money in my purse, and nothing particular to interest me on shore, I thought I would sail about a little and see the watery part of the world. It is a way I have of driving off the spleen and regulating the circulation. Whenever I find myself growing grim about the mouth; whenever it is a damp, drizzly November in my soul; whenever I find myself involuntarily pausing before coffin warehouses, and bringing up the rear of every funeral I meet; and especially whenever my hypos get such an upper hand of me, that it requires a strong moral principle to prevent me from deliberately stepping into the street, and methodically knocking people’s hats off—then, I account it high time to get to sea as soon as I can. This is my substitute for pistol and ball. With a philosophical flourish Cato throws himself upon his sword; I quietly take to the ship. There is nothing surprising in this. If they but knew it, almost all men in their degree, some time or other, cherish very nearly the same feelings towards the ocean with me.']...\n",
            "Final feedback: Detected Issue: LABEL_0. Consider revising.\n",
            "Final refined text: ['Sailing is a way I have of driving off the spleen and regulating the circulation. Cato throws himself upon his sword; I quietly take to the ship. Almost all men in their degree, some time or other, cherish very nearly the same feelings towards the ocean with me.']...\n",
            "Rouge Score: 0.5569180521085637\n"
          ]
        }
      ]
    }
  ]
}